[{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1 - English Reverse Translation","tags":[],"description":"","content":"How AWS and Intel make Large Language Models more accessible and cost-effective with DeepSeek by Dylan Souvage, Vishwa Gopinath Kurakundi, and Anish Kumar on 07 APRIL 2025 in Amazon EC2, Artificial Intelligence, Compute, Generative AI, Generative AI, Partner solutions, Responsible AI Permalink\nBy Anish Kumar, AI Software Engineering Manager – Intel Dylan Souvage, Solutions Architect – AWS Vishwa Gopinath Kurakundi, Solutions Architect – AWS\nEnterprises are seeking effective ways to deploy Large Language Models (LLMs). They want to leverage the power of LLMs while also needing solutions that balance performance and cost.\nAt the recent AWS re:Invent conference in Las Vegas, Andy Jassy, CEO of Amazon, shared three valuable lessons from Amazon\u0026rsquo;s internal experience building more than 1,000 GenAI applications:\nCost-effectiveness at scale is key for GenAI applications. Building effective GenAI applications requires careful consideration. Model diversity is essential – there is no \u0026ldquo;one model fits all\u0026rdquo; solution. These lessons guide how AWS collaborates with customers to deploy GenAI. At AWS, we recognize that flexibility and choice are important for customers. Andy Jassy also emphasized that AWS\u0026rsquo;s diverse LLM portfolio helps customers easily find the right tool for their specific needs. Through deep collaboration with partners like Intel, AWS continuously expands its curated LLM portfolio, enhancing accessibility for customers.\nIntel and AWS The collaboration between AWS and Intel began in 2006, when we launched Amazon Elastic Compute Cloud (EC2) using Intel chips. Over 19 years, this partnership has grown strong to provide cloud services that optimize costs, simplify operations, and meet changing business needs. Intel® Xeon® Scalable processors are the foundation for many cloud computing services on AWS. EC2 instances using Intel Xeon processors have the broadest availability, global reach, and highest availability in AWS regions. In September 2024, AWS and Intel announced a multi-year, multi-billion dollar joint investment agreement to design custom chips including products and wafers from Intel. This extends the long-term collaboration between the two companies, helping customers run nearly any type of workload and accelerate the performance of artificial intelligence (AI) applications.\nDeepSeek AWS and Intel are collaborating to help enterprises access and deploy cost-effective LLMs. A new trend is distilled language models, which maintain high performance while requiring fewer resources. These models can run directly on CPUs, also known as Small Language Models (SLMs). Training and inferring SLMs on CPUs helps deploy high-performance AI within reasonable time and cost constraints. DeepSeek models (developer of DeepSeek-R1) are rapidly becoming popular due to their efficiency, low cost, and open-source nature, allowing free deployment in applications. Additionally, DeepSeek provides distilled versions, smaller \u0026ldquo;student\u0026rdquo; models trained to replicate the response quality of larger \u0026ldquo;teacher\u0026rdquo; models but consuming fewer resources.\nAmazon EC2 is a cost-effective platform for deploying Large Language Models (LLMs), while providing specialized instance types running on Intel® Xeon® Scalable processors, suitable for deploying optimized models like the DeepSeek-R1 distilled version. 4th generation and newer Intel® Xeon® CPUs are equipped with Advanced Matrix Extensions (AMX) accelerators, significantly improving LLM workload performance by accelerating matrix multiplication, a core component in LLM inference. These AMX accelerators deliver superior processing performance while integrating with open standards like oneAPI, helping enterprises deploy Generative Artificial Intelligence (Generative AI) applications with reasonable costs, high scalability, faster result acquisition time, and lower total cost of ownership (TCO).\nAmazon EC2 also provides excellent flexibility and scalability by supporting various deployment configurations, including virtual LLM (vLLM) models that can seamlessly integrate with Docker based on the Hugging Face platform. In this accompanying tutorial, we\u0026rsquo;ll see in detail step-by-step how to quickly deploy the DeepSeek-R1-Distill-Llama-8B model on Amazon EC2 m7i.2xlarge instance, using Intel® Xeon® Scalable processor with 8 vCPU and 32 GB memory. The article provides detailed guidance on configuring Amazon EC2 to deploy the model, while building Docker container for vLLM on CPU – including Intel\u0026rsquo;s CPU optimizations like Intel Extension for PyTorch. This extension ensures LLM inference processes are optimized to run efficiently on 4th generation and newer Intel® Xeon® processors, and the article concludes with testing inference after the model is deployed.\nConclusion Enterprises can deploy custom or open-source LLMs, including Distilled DeepSeek-R1, on AWS through managed services like Amazon Bedrock and Amazon SageMaker, or deploy directly on Amazon EC2, depending on specific needs. The collaboration between AWS and Intel is driving the development of the Generative Artificial Intelligence field, combining Intel\u0026rsquo;s advanced semiconductor technology with AWS\u0026rsquo;s powerful cloud infrastructure to deliver accessible and cost-effective AI solutions.\nTo learn more about AWS\u0026rsquo;s Generative AI field, visit AWS\u0026rsquo;s Machine Learning blog.\n![Connect with Intel][image1]\nIntel – AWS Partner Spotlight Intel and Amazon Web Services (AWS) have collaborated for over 19 years to develop flexible and optimized software technologies, serving enterprise critical workloads. This collaboration enables AWS partners to support customers in migrating and modernizing their applications and infrastructure – helping reduce costs and complexity, accelerate business outcomes, while scaling to meet current and future computing needs. Contact Intel | Partner Overview | AWS Marketplace\nTAGS: Amazon EC2, AWS Competency Partners, AWS Partner Solution, Generative AI, Intel\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nSummary Report: “GenAI-powered App-DB Modernization workshop” Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the “GenAI-powered App-DB Modernization” workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Add your event photos here\nOverall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Tran Trong Nghia\nPhone Number: 0865366219\nEmail: hs23t2@gmail.com\nUniversity: FPT University\nMajor: Software Engineering\nClass: OJT\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Complete Module 01: Basic Knowledge of AWS Understand cloud computing concepts and AWS global infrastructure Set up AWS account with proper security configurations Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Orientation and internship introduction - Review AWS training roadmap - Set up development environment 09/08/2025 09/08/2025 FCJ Orientation 2 - Study Module 01-01: What is Cloud Computing - Benefits of Cloud Computing - What makes AWS different 09/09/2025 09/09/2025 AWS Cloud Journey 3 - Study Module 01-02: AWS Global Infrastructure - Data Centers, Availability Zones, Regions - Edge Locations 09/10/2025 09/10/2025 AWS Cloud Journey 4 - Study Module 01-03: AWS Management Tools - AWS Console, CLI, SDK - AWS Cost Optimization 09/11/2025 09/11/2025 AWS Cloud Journey 5 - Study Module 01-04: AWS Support - Support Plans - Trusted Advisor 09/12/2025 09/12/2025 AWS Cloud Journey 6-7 - Lab Practice: Module 01-Lab01 - Create AWS account - Setup MFA - Create admin users 09/13-14/2025 09/14/2025 AWS Cloud Journey Week 1 Achievements: Understood fundamental cloud computing concepts and the advantages of cloud adoption Learned about AWS\u0026rsquo;s global infrastructure including Regions, Availability Zones, and Edge Locations Gained knowledge of various tools for managing AWS services (Console, CLI, SDK) Understood cost optimization strategies and AWS pricing models Learned about different AWS support plans and when to use each Successfully created AWS account with proper security configurations (MFA enabled) Created admin group and users following AWS best practices Generated access keys for programmatic access Prepared foundation for advanced AWS studies Challenges Faced: Payment method verification during AWS account creation Understanding the difference between various AWS support tiers Solutions Implemented: Used international credit card for AWS account verification Created comparison matrix of AWS support plans for better understanding Next Week\u0026rsquo;s Plan: Begin Module 02: AWS VPC and Security Study VPC concepts, subnets, and networking components Learn about security groups and NACLs "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"AWS Training \u0026amp; Project Worklog This worklog documents my 12-week journey through the AWS training program and final project development. Starting from September 8th, 2025, I completed comprehensive AWS training covering 7 modules, followed by intensive research and development of a full-stack serverless application.\nWeekly Breakdown Week 1 (Sep 8-14): AWS Fundamentals \u0026amp; Account Setup\nWeek 2 (Sep 15-21): VPC \u0026amp; Networking\nWeek 3 (Sep 22-28): Amazon EC2 - Core Compute\nWeek 4 (Sep 29 - Oct 5): EC2 Advanced \u0026amp; Auto Scaling\nWeek 5 (Oct 6-12): AWS Storage Services \u0026amp; Blog Translation\nWeek 6 (Oct 13-19): Security, IAM \u0026amp; Review\nWeek 7 (Oct 20-26): Database Services\nWeek 8 (Oct 27 - Nov 2): Data Analytics \u0026amp; Project Kickoff\nWeek 9 (Nov 3-9): AWS Services Research\nWeek 10 (Nov 10-16): Architecture Design\nWeek 11 (Nov 17-23): Backend Implementation\nWeek 12 (Nov 24-30): Frontend \u0026amp; Production Deployment\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2 - English Reverse Translation","tags":[],"description":"","content":"How AWS Skill Builder powers digital skills growth across European institutions Developing cloud computing skills together: AWS\u0026rsquo;s specialized support for EU public sector digital transformation Public sector organizations across the European Union are accelerating their digital transformation to better serve citizens. With the European Commission\u0026rsquo;s Declaration on Digital Rights and Principles establishing a framework for the digital transformation, EU member states have a shared vision to create a digitally-enabled, single market that works for everyone. As the public sector increasingly adopts cloud technology to modernize and scale operations, there\u0026rsquo;s an urgent need to develop the digital skills necessary to use cloud technology effectively.\n[Omitted long context line]\n[Omitted long context line]\nLeading digital skills development for EU governments Building on that success, AWS has created three additional learning paths:\n[Omitted long context line]\nAnnouncing Cloud Coach sessions for EU governments [Omitted long context line]\n[Omitted long context line]\nConnecting with our EU public sector learning team [Omitted long context line]\nTo learn more about AWS training and certifications for government organizations, visit the dedicated page AWS Training for European Government Agencies and Organizations.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Complete Module 02: AWS VPC and Security Understand Virtual Private Cloud concepts and components Learn about network security and connectivity options Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Module 02-01: AWS VPC Introduction - VPC concepts, Public/Private subnets - CIDR blocks and IP addressing 09/15/2025 09/15/2025 AWS Cloud Journey 2 - Study Module 02-02: VPC Components - Route Tables, ENI, EIP - Internet Gateway, NAT Gateway 09/16/2025 09/16/2025 AWS Cloud Journey 3 - Study Module 02-03: VPC Security - Security Groups vs NACLs - Best practices 09/17/2025 09/17/2025 AWS Cloud Journey 4 - Study Network Connectivity - VPN Site-to-Site - Direct Connect, Load Balancers 09/18/2025 09/18/2025 AWS Cloud Journey 5 - Lab Practice: Module 02-Lab03 - Create VPC and subnets - Configure Internet Gateway 09/19/2025 09/19/2025 AWS Cloud Journey 6-7 - Continue Lab Practice - Set up routing - Test network connectivity 09/20-21/2025 09/21/2025 AWS Cloud Journey Week 2 Achievements: Mastered VPC concepts including public vs private subnets Understanding of VPC components: Route Tables, ENI, EIP, VPC Endpoints Learned about network connectivity options: IGW, NAT Gateway, VPN Differentiated between Security Groups (stateful) and NACLs (stateless) Successfully created a custom VPC with proper subnet configuration Configured Internet Gateway for public subnet access Set up routing tables for proper traffic flow Tested network connectivity between components Challenges Faced: Confusion between public and private subnet routing Understanding when to use Security Groups vs NACLs Configuring proper routing tables Solutions Implemented: Created routing diagrams to visualize traffic flow Built comparison table for Security Groups vs NACLs Step-by-step routing table configuration guide Next Week\u0026rsquo;s Plan: Begin Module 03: Amazon EC2 Study EC2 instance types and features Learn about AMI, EBS, and storage options "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nIn this section, you need to summarize the contents of the workshop that you plan to conduct.\nIoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The IoT Weather Platform is designed for the ITea Lab team in Ho Chi Minh City to enhance weather data collection and analysis. It supports up to 5 weather stations, with potential scalability to 10-15, utilizing Raspberry Pi edge devices with ESP32 sensors to transmit data via MQTT. The platform leverages AWS Serverless services to deliver real-time monitoring, predictive analytics, and cost efficiency, with access restricted to 5 lab members via Amazon Cognito.\n2. Problem Statement What’s the Problem? Current weather stations require manual data collection, becoming unmanageable with multiple units. There is no centralized system for real-time data or analytics, and third-party platforms are costly and overly complex.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, Amazon S3 for storage (including a data lake), and AWS Glue Crawlers and ETL jobs to extract, transform, and load data from the S3 data lake to another S3 bucket for analysis. AWS Amplify with Next.js provides the web interface, and Amazon Cognito ensures secure access. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time dashboards, trend analysis, and low operational costs.\nBenefits and Return on Investment The solution establishes a foundational resource for lab members to develop a larger IoT platform, serving as a study resource, and provides a data foundation for AI enthusiasts for model training or analysis. It reduces manual reporting for each station via a centralized platform, simplifying management and maintenance, and improves data reliability. Monthly costs are $0.66 USD per the AWS Pricing Calculator, with a 12-month total of $7.92 USD. All IoT equipment costs are covered by the existing weather station setup, eliminating additional development expenses. The break-even period of 6-12 months is achieved through significant time savings from reduced manual work.\n3. Solution Architecture The platform employs a serverless AWS architecture to manage data from 5 Raspberry Pi-based stations, scalable to 15. Data is ingested via AWS IoT Core, stored in an S3 data lake, and processed by AWS Glue Crawlers and ETL jobs to transform and load it into another S3 bucket for analysis. Lambda and API Gateway handle additional processing, while Amplify with Next.js hosts the dashboard, secured by Cognito. The architecture is detailed below:\nAWS Services Used AWS IoT Core: Ingests MQTT data from 5 stations, scalable to 15. AWS Lambda: Processes data and triggers Glue jobs (two functions). Amazon API Gateway: Facilitates web app communication. Amazon S3: Stores raw data in a data lake and processed outputs (two buckets). AWS Glue: Crawlers catalog data, and ETL jobs transform and load it. AWS Amplify: Hosts the Next.js web interface. Amazon Cognito: Secures access for lab users. Component Design Edge Devices: Raspberry Pi collects and filters sensor data, sending it to IoT Core. Data Ingestion: AWS IoT Core receives MQTT messages from the edge devices. Data Storage: Raw data is stored in an S3 data lake; processed data is stored in another S3 bucket. Data Processing: AWS Glue Crawlers catalog the data, and ETL jobs transform it for analysis. Web Interface: AWS Amplify hosts a Next.js app for real-time dashboards and analytics. User Management: Amazon Cognito manages user access, allowing up to 5 active accounts. 4. Technical Implementation Implementation Phases This project has two parts—setting up weather edge stations and building the weather platform—each following 4 phases:\nBuild Theory and Draw Architecture: Research Raspberry Pi setup with ESP32 sensors and design the AWS serverless architecture (1 month pre-internship) Calculate Price and Check Practicality: Use AWS Pricing Calculator to estimate costs and adjust if needed (Month 1). Fix Architecture for Cost or Solution Fit: Tweak the design (e.g., optimize Lambda with Next.js) to stay cost-effective and usable (Month 2). Develop, Test, and Deploy: Code the Raspberry Pi setup, AWS services with CDK/SDK, and Next.js app, then test and release to production (Months 2-3). Technical Requirements\nWeather Edge Station: Sensors (temperature, humidity, rainfall, wind speed), a microcontroller (ESP32), and a Raspberry Pi as the edge device. Raspberry Pi runs Raspbian, handles Docker for filtering, and sends 1 MB/day per station via MQTT over Wi-Fi. Weather Platform: Practical knowledge of AWS Amplify (hosting Next.js), Lambda (minimal use due to Next.js), AWS Glue (ETL), S3 (two buckets), IoT Core (gateway and rules), and Cognito (5 users). Use AWS CDK/SDK to code interactions (e.g., IoT Core rules to S3). Next.js reduces Lambda workload for the fullstack web app. 5. Timeline \u0026amp; Milestones Project Timeline\nPre-Internship (Month 0): 1 month for planning and old station review. Internship (Months 1-3): 3 months. Month 1: Study AWS and upgrade hardware. Month 2: Design and adjust architecture. Month 3: Implement, test, and launch. Post-Launch: Up to 1 year for research. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services: AWS Lambda: $0.00/month (1,000 requests, 512 MB storage). S3 Standard: $0.15/month (6 GB, 2,100 requests, 1 GB scanned). Data Transfer: $0.02/month (1 GB inbound, 1 GB outbound). AWS Amplify: $0.35/month (256 MB, 500 ms requests). Amazon API Gateway: $0.01/month (2,000 requests). AWS Glue ETL Jobs: $0.02/month (2 DPUs). AWS Glue Crawlers: $0.07/month (1 crawler). MQTT (IoT Core): $0.08/month (5 devices, 45,000 messages). Total: $0.7/month, $8.40/12 months\nHardware: $265 one-time (Raspberry Pi 5 and sensors). 7. Risk Assessment Risk Matrix Network Outages: Medium impact, medium probability. Sensor Failures: High impact, low probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies Network: Local storage on Raspberry Pi with Docker. Sensors: Regular checks and spares. Cost: AWS budget alerts and optimization. Contingency Plans Revert to manual methods if AWS fails. Use CloudFormation for cost-related rollbacks. 8. Expected Outcomes Technical Improvements: Real-time data and analytics replace manual processes.\nScalable to 10-15 stations.\nLong-term Value 1-year data foundation for AI research.\nReusable for future projects.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3 - English Reverse Translation","tags":[],"description":"","content":"Simultaneously serving multiple Large Language Models (LLMs) with LoRAX [Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n![][image1]\nWhy choose LoRAX for LoRA deployment on AWS? The growing popularity of LLM fine-tuning has led to the emergence of various inference container methods for deploying LoRA adapters on AWS. The two most prominent methods used by our customers are LoRAX and vLLM.\n[Omitted long context line]\n[Omitted long context line]\nSolution Overview The LoRAX inference container can be deployed on a single EC2 G6 instance, and models along with adapters can be loaded from Amazon Simple Storage Service (Amazon S3) or Hugging Face. The following diagram illustrates the architecture of this solution.\n![][image3]\nPrerequisites To follow this tutorial, you need access to the following requirements:\nAn AWS account Appropriate permissions to deploy EC2 G6 instances. LoRAX is built with the purpose of using NVIDIA CUDA technology, and the EC2 G6 instance family is the most cost-effective type with the latest NVIDIA CUDA accelerators. Specifically, G6.xlarge is the most cost-effective choice for this tutorial at the time of writing. Please ensure that the quota has been increased before deployment. [Omitted long context line] Hands-on Tutorial This article will guide you through creating an EC2 instance, downloading and deploying the container image, while storing a pre-trained language model along with custom adapters from Amazon S3. Follow the prerequisite checklist to ensure you can deploy this solution correctly.\nDetailed Server Configuration [Omitted long matching line]\n[Omitted long matching line]\nDepending on the parameters of the language model, you need to adjust the Amazon Elastic Block Store (Amazon EBS) storage capacity to have enough space for the base model and adapter weights.\nTo set up your inference server, follow these steps:\nIn the Amazon EC2 management console, select Launch instances, as illustrated in the image below.![][image4] For the name, enter LoRAX - Inference Server. To open AWS CloudShell, in the lower left corner of the AWS Management Console select CloudShell, as illustrated in the following screenshot.![][image5] [Omitted long matching line]\n[Omitted long matching line]\nCost Comparison and Scalability Advice [Omitted long context line]\nIn the previous example, the adapters obtained from the training process have a size of approximately 5 MB.\n[Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\nStoring LoRAX server for multiple models in production environment [Omitted long context line]\n![][image17]\nCleanup In this tutorial, we have created security groups, an S3 bucket, a SageMaker notebook instance (optional), and an EC2 inference server. It\u0026rsquo;s important to terminate the resources created during the process to avoid incurring additional costs:\nDelete the S3 bucket Terminate the EC2 inference server Terminate the SageMaker notebook instance Conclusion [Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n[Omitted long context line]\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Complete Module 03: Amazon EC2 (Part 1) Understand EC2 instance types and purchasing options Learn about EC2 storage and networking features Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Module 03-01-01: EC2 Overview - EC2 instance types - Hypervisor concepts 09/22/2025 09/22/2025 AWS Cloud Journey 2 - Study Module 03-01-02: AMI \u0026amp; Key Pairs - Amazon Machine Images - Key Pair management 09/23/2025 09/23/2025 AWS Cloud Journey 3 - Study Module 03-01-03: EBS - Elastic Block Store - HDD vs SSD types 09/24/2025 09/24/2025 AWS Cloud Journey 4 - Study Module 03-01-04: Instance Store - Instance Store vs EBS - Use cases 09/25/2025 09/25/2025 AWS Cloud Journey 5 - Lab Practice: Module 03-Lab01 - Launch EC2 instances - Configure storage 09/26/2025 09/26/2025 AWS Cloud Journey 6-7 - Continue Lab Practice - Test different instance types - Backup and restore 09/27-28/2025 09/28/2025 AWS Cloud Journey Week 3 Achievements: Understood EC2 instance types and when to use each family Learned about AMIs for customizing EC2 instances Mastered Key Pair creation and SSH access Differentiated between EBS and Instance Store Understanding of EBS volume types and performance Successfully launched and configured EC2 instances Attached and configured EBS volumes Created AMIs for backup purposes Tested SSH connectivity using key pairs Challenges Faced: Selecting appropriate instance types for workloads Understanding the difference between EBS and Instance Store Configuring proper storage performance Solutions Implemented: Created decision matrix for EC2 instance selection Built comparison chart for storage options Performance testing methodology for EBS volumes Next Week\u0026rsquo;s Plan: Continue Module 03: Amazon EC2 (Part 2) Study User Data and Metadata Learn about Auto Scaling and Load Balancing "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Complete Module 03: Amazon EC2 (Part 2) Understand advanced EC2 features Learn about Auto Scaling and related services Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Module 03-01-05: User Data - EC2 User Data scripts - Bootstrap instances 09/29/2025 09/29/2025 AWS Cloud Journey 2 - Study Module 03-01-06: Metadata - EC2 Instance Metadata - Dynamic configuration 09/30/2025 09/30/2025 AWS Cloud Journey 3 - Study Module 03-01-07: Auto Scaling - Auto Scaling Groups - Scaling policies 10/01/2025 10/01/2025 AWS Cloud Journey 4 - Study Module 03-02: Advanced Services - EFS, FSx, Lightsail - Migration services 10/02/2025 10/02/2025 AWS Cloud Journey 5 - Lab Practice: Module 03-Lab13 - Configure Auto Scaling - Test scaling policies 10/03/2025 10/03/2025 AWS Cloud Journey 6-7 - Continue Lab Practice - Work with EFS - Performance testing 10/04-05/2025 10/05/2025 AWS Cloud Journey Week 4 Achievements: Mastered EC2 User Data for instance bootstrap Learned to use Instance Metadata for dynamic configuration Understanding of Auto Scaling Groups and policies Knowledge of EFS for shared storage across instances Exposure to migration services (MGN) Successfully configured Auto Scaling Group Implemented scaling policies based on metrics Tested EFS file system with multiple EC2 instances Created disaster recovery scenarios Challenges Faced: Debugging User Data scripts Configuring proper Auto Scaling policies Understanding EFS performance characteristics Solutions Implemented: Created User Data script templates Built step-by-step Auto Scaling configuration guide Performance testing methodology for EFS Next Week\u0026rsquo;s Plan: Begin Module 04: AWS Storage Services Focus on S3 and storage classes Prepare for blog translation on Oct 10th "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim for your report, including this warning.\nIn this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Complete Module 04: AWS Storage Services Blog Translation on October 10th Review concepts covered so far Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Module 04-01: Storage Overview - S3, Storage Gateway, Snow Family - Disaster Recovery concepts 10/06/2025 10/06/2025 AWS Cloud Journey 2 - Study Module 04-02: S3 Deep Dive - S3 Buckets, Access Points - Storage Classes - Blog Translation 10/07/2025 10/07/2025 AWS Cloud Journey 3 - Study Module 04-03: S3 Advanced - S3 Static Websites - CORS, Versioning, Glacier - Blog Translation 10/08/2025 10/08/2025 AWS Cloud Journey 4 - Study Module 04-04: Hybrid Storage - Storage Gateway - Snow Family, AWS Backup - Blog Translation 10/09/2025 10/09/2025 AWS Cloud Journey 5 BLOG TRANSLATION DAY - Translate assigned blog content - Review and edit translation 10/10/2025 10/10/2025 Blog Translation 6-7 - Lab Practice: Module 04-Lab13 - S3 static website setup - Configure CloudFront 10/11-12/2025 10/12/2025 AWS Cloud Journey Week 5 Achievements: Understanding of AWS Storage services portfolio Mastered S3 concepts and storage classes Knowledge of S3 static website hosting Understanding of hybrid storage solutions Successfully translated blog content Configured S3 bucket for static website Set up CloudFront distribution Implemented CORS policies Tested S3 versioning and lifecycle policies Challenges Faced: Understanding S3 storage class transitions Configuring proper CORS policies Technical terminology in blog translation Solutions Implemented: Created S3 storage class decision tree Built CORS configuration templates Technical glossary for translation consistency Next Week\u0026rsquo;s Plan: Begin Module 05: AWS Security \u0026amp; IAM Study Shared Responsibility Model Learn about IAM, Cognito, and Organizations "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Complete Module 05: AWS Security \u0026amp; IAM Review all concepts from Modules 1-5 Research for final project Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Module 05-01: Shared Responsibility Model - AWS vs Customer responsibilities - Service model considerations 10/13/2025 10/13/2025 AWS Cloud Journey 2 - Study Module 05-02: IAM Deep Dive - Users, Groups, Roles, Policies - Best practices 10/14/2025 10/14/2025 AWS Cloud Journey 3 - Study Module 05-03: Amazon Cognito - User Pools vs Identity Pools - Federation 10/15/2025 10/15/2025 AWS Cloud Journey 4 - Study Module 05-04: AWS Organizations - Multi-account management - Service Control Policies 10/16/2025 10/16/2025 AWS Cloud Journey 5 - Lab Practice: Module 05-Lab18 - AWS Security Hub setup - Security compliance 10/17/2025 10/17/2025 AWS Cloud Journey 6-7 REVIEW \u0026amp; RESEARCH - Review Modules 1-5 concepts - Research final project ideas 10/18-19/2025 10/19/2025 Study Materials Week 6 Achievements: Understanding of AWS Shared Responsibility Model Mastered IAM concepts and best practices Knowledge of Cognito for user authentication Understanding of multi-account management with Organizations Successfully configured Security Hub Reviewed all concepts from first 5 modules Researched potential final project architectures Created study notes for exam preparation Challenges Faced: Understanding IAM policy evaluation logic Differentiating Cognito User Pool vs Identity Pool Choosing appropriate final project scope Solutions Implemented: Created IAM policy decision flowchart Built comparison table for Cognito features Documented final project requirements and constraints Next Week\u0026rsquo;s Plan: Begin Module 06: AWS Database Services Study RDS, Aurora, and DynamoDB Continue final project research "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Complete Module 06: AWS Database Services Understand database concepts and AWS database offerings Continue final project research Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Module 06-01: Database Concepts - SQL vs NoSQL - OLTP vs OLAP 10/20/2025 10/20/2025 AWS Cloud Journey 2 - Study Module 06-02: RDS \u0026amp; Aurora - Amazon RDS features - Aurora advantages 10/21/2025 10/21/2025 AWS Cloud Journey 3 - Study Module 06-03: Redshift \u0026amp; ElastiCache - Data warehousing - Caching strategies 10/22/2025 10/22/2025 AWS Cloud Journey 4 - Lab Practice: Module 06-Lab05 - Create RDS instance - Configure backup 10/23/2025 10/23/2025 AWS Cloud Journey 5 - Lab Practice continued - Connect EC2 to RDS - Test failover 10/24/2025 10/24/2025 AWS Cloud Journey 6-7 FINAL PROJECT RESEARCH - Design project architecture - Create project plan 10/25-26/2025 10/26/2025 Project Resources Week 7 Achievements: Understanding of database fundamentals Knowledge of AWS RDS and Aurora features Understanding of data warehousing with Redshift Knowledge of caching with ElastiCache Successfully created and configured RDS instance Implemented backup and restore procedures Designed final project architecture Created detailed project implementation plan Challenges Faced: Choosing between different database engines Understanding Multi-AZ configurations Designing scalable project architecture Solutions Implemented: Created database selection decision matrix Built RDS configuration checklist Documented project architecture with diagrams Next Week\u0026rsquo;s Plan: Begin Module 07: Data Analytics \u0026amp; Lake House Study Glue, Athena, and QuickSight Start implementing final project "},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nHere, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Complete Module 07: Data Analytics \u0026amp; Lake House Understand data lake concepts and services Begin final project implementation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Study Data Lake Concepts - Glue, Athena, QuickSight overview - Data pipeline architecture 10/27/2025 10/27/2025 AWS Cloud Journey 2 - Study Module 07-01: DynamoDB - NoSQL concepts - DynamoDB features 10/28/2025 10/28/2025 AWS Cloud Journey 3 - Lab Practice: Module 07-Lab35 - Create S3 Data Lake - Set up Glue Crawler 10/29/2025 10/29/2025 AWS Cloud Journey 4 - Lab Practice continued - Athena queries - QuickSight visualization 10/30/2025 10/30/2025 AWS Cloud Journey 5 FINAL PROJECT IMPLEMENTATION - Set up project infrastructure - Configure core services 10/31/2025 10/31/2025 Project Resources 6-7 PROJECT WORK - Implement main features - Test functionality 11/01-02/2025 11/02/2025 Project Resources Week 8 Achievements: Understanding of Data Lake architecture Knowledge of AWS Analytics services Hands-on experience with Glue, Athena, QuickSight Understanding of DynamoDB NoSQL database Successfully created Data Lake pipeline Implemented data catalog with Glue Created visualizations with QuickSight Started final project implementation Challenges Faced: Understanding Data Lake vs Data Warehouse Configuring proper IAM for analytics services Debugging data transformations Solutions Implemented: Created Data Lake architecture diagrams Built IAM policy templates for analytics Step-by-step data pipeline guide Next Week\u0026rsquo;s Plan: Complete final project implementation Document project architecture Prepare for presentation "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Research AWS Advanced Services for Final Project Deep dive into security, CI/CD, and infrastructure services Evaluate tools for project implementation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Amazon Cognito Deep Dive - User pools vs Identity pools - Authentication flows - Social integration 11/03/2025 11/03/2025 AWS Docs 2 AWS WAF \u0026amp; Security - WAF rules and ACLs - Security best practices - DDoS protection 11/04/2025 11/04/2025 AWS Docs 3 Developer Tools Research - CodeCommit, CodeBuild, CodePipeline - CodeStar overview - CI/CD workflows 11/05/2025 11/05/2025 AWS Docs 4 Infrastructure as Code - Terraform fundamentals - CloudFormation vs Terraform - IaC best practices 11/06/2025 11/06/2025 Terraform Docs 5 AWS Amplify \u0026amp; Lambda - Amplify framework - Lambda deep dive - Serverless patterns 11/07/2025 11/07/2025 AWS Docs 6-7 Global Services - Route 53 configuration - CloudFront strategies - S3 advanced patterns 11/08-09/2025 11/09/2025 AWS Docs Week 9 Achievements: Comprehensive understanding of AWS Cognito authentication patterns Knowledge of WAF configuration for web application security Understanding of AWS CI/CD pipeline components Basic proficiency in Terraform for infrastructure management Awareness of Amplify capabilities for full-stack development Knowledge of Route 53 for DNS management Understanding of CloudFront optimization strategies Advanced S3 patterns for different use cases Challenges Faced: Complexity of Cognito configuration Terraform learning curve CI/CD pipeline design decisions Solutions Implemented: Created Cognito configuration templates Built Terraform module examples Designed sample CI/CD workflows Documented WAF rule patterns Next Week\u0026rsquo;s Plan: Design final project architecture Create detailed implementation plan Set up development environment "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Design Final Project Architecture Plan infrastructure deployment strategy Create detailed technical specifications Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Architecture Planning - Design system architecture - Define microservices structure - Create data flow diagrams 11/10/2025 11/10/2025 Architecture Docs 2 Infrastructure Design - Plan VPC architecture - Design security layers - Plan CI/CD pipeline 11/11/2025 11/11/2025 AWS Well-Architected 3 Database Architecture - Design data models - Plan RDS/DynamoDB usage - Design caching strategy 11/12/2025 11/12/2025 Database Patterns 4 Security Architecture - Design authentication flow with Cognito - Plan WAF rules - Design IAM structure 11/13/2025 11/13/2025 Security Best Practices 5 CDN \u0026amp; Global Design - Plan CloudFront distribution - Design Route 53 strategy - Plan S3 bucket structure 11/14/2025 11/14/2025 AWS Patterns 6-7 Create Terraform Scripts - Write infrastructure code - Create modular design - Plan deployment stages 11/15-16/2025 11/16/2025 Terraform Week 10 Achievements: Complete system architecture documentation Detailed infrastructure design with security considerations Comprehensive data architecture plan Terraform scripts for infrastructure deployment CI/CD pipeline design using AWS CodePipeline Security architecture with WAF and Cognito integration Global deployment strategy with CloudFront and Route 53 Modular infrastructure code for reusability Challenges Faced: Balancing complexity vs scalability Security vs accessibility trade-offs Cost optimization in design Solutions Implemented: Created architecture decision records (ADRs) Built cost estimation models Designed phased deployment approach Created infrastructure testing strategy Next Week\u0026rsquo;s Plan: Start coding backend services Implement core infrastructure with Terraform Set up CI/CD pipeline "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Backend Services Development Lambda Function Implementation CI/CD Pipeline Setup Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Infrastructure Deployment - Apply Terraform scripts - Set up VPC and security groups - Configure S3 buckets 11/17/2025 11/17/2025 Terraform 2 Database Setup - Deploy RDS instances - Configure DynamoDB tables - Set up ElastiCache 11/18/2025 11/18/2025 AWS Console 3 Lambda Development - Create Lambda functions - Implement API endpoints - Set up API Gateway 11/19/2025 11/19/2025 VS Code 4 Cognito Integration - Set up User Pool - Configure authentication - Implement auth in Lambda 11/20/2025 11/20/2025 AWS Console 5 CodeCommit Setup - Initialize repositories - Push initial code - Set up branch strategy 11/21/2025 11/21/2025 Git 6-7 CI/CD Pipeline - Configure CodeBuild - Set up CodePipeline - Test automated deployments 11/22-23/2025 11/23/2025 AWS Code* Week 11 Achievements: Successfully deployed infrastructure using Terraform Fully configured database layer with RDS and DynamoDB Implemented backend services with Lambda Integrated Cognito authentication system Set up version control with CodeCommit Built working CI/CD pipeline with automated deployments Created modular Lambda functions for different services Implemented proper error handling and logging Challenges Faced: Terraform state management Lambda cold start optimization CI/CD pipeline debugging Cognito configuration complexity Solutions Implemented: Used remote state storage for Terraform Implemented Lambda best practices for performance Created pipeline testing strategies Built Cognito configuration templates Next Week\u0026rsquo;s Plan: Frontend development with Amplify Testing and quality assurance Performance optimization Documentation and deployment "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Frontend Development with Amplify Testing and Quality Assurance Performance Optimization and Deployment Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 Amplify Setup - Initialize Amplify project - Configure auth with Cognito - Set up GraphQL API 11/24/2025 11/24/2025 Amplify Docs 2 Frontend Development - Build React/Vue components - Integrate with API Gateway - Implement authentication flow 11/25/2025 11/25/2025 React/Vue 3 CloudFront \u0026amp; Route 53 - Configure CloudFront distribution - Set up custom domain - Configure Route 53 11/26/2025 11/26/2025 AWS Console 4 WAF Implementation - Set up WAF rules - Configure SQL injection protection - Set up rate limiting 11/27/2025 11/27/2025 AWS WAF 5 Testing \u0026amp; QA - Unit testing - Integration testing - Load testing 11/28/2025 11/28/2025 Testing Tools 6-7 Final Deployment - Production deployment - Monitoring setup - Documentation completion 11/29-30/2025 11/30/2025 AWS Week 12 Achievements: Successfully deployed full-stack application with Amplify Integrated authentication with Cognito Configured global CDN with CloudFront Set up custom domain with Route 53 Implemented WAF for security Completed comprehensive testing Optimized application performance Deployed to production environment Created complete project documentation Challenges Faced: Amplify configuration complexity Frontend-backend integration DNS propagation delays Performance optimization Solutions Implemented: Used Amplify CLI for streamlined setup Created API integration layer Planned DNS changes in advance Implemented caching strategies Project Summary: Built complete serverless application Used modern AWS services (Lambda, DynamoDB, Cognito) Implemented CI/CD with AWS CodePipeline Achieved global scalability with CloudFront Secured application with WAF Documented entire architecture and deployment process "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]